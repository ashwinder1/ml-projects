{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinder1/ml-projects/blob/main/text_classification_hackathon_ver3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIf5K1pcbI18"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1lUMPjqe_K6"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/IndiaAI-Hackathon/dataset/train.csv'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQlcROvlfB2O"
      },
      "outputs": [],
      "source": [
        "# read the csv file using pandas and see the header rows\n",
        "df = pd.read_csv('/content/drive/MyDrive/IndiaAI-Hackathon/dataset/train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YK6WFaUAMVi"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8lVoRqxARfq"
      },
      "outputs": [],
      "source": [
        "print(len(df['crimeaditionalinfo']))\n",
        "print(len(df['sub_category']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmpLp6MV6Bzb"
      },
      "source": [
        "# 1. Text Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjfzjR7Z6L-x"
      },
      "source": [
        "- convert to lowercase\n",
        "- remove extra spaces\n",
        "- remove Nan values\n",
        "- convert to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE4PAEty_8rh",
        "outputId": "a4f0b473-eda1-45f0-b78b-6ed1b6dd8edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93686\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ... (other imports and code)\n",
        "\n",
        "cases_data = df['crimeaditionalinfo'] # get the description data or features\n",
        "\n",
        "# Handle NaN values and convert to string\n",
        "cases_data = cases_data.fillna(\"\").astype(str)\n",
        "\n",
        "# Convert to lowercase\n",
        "cases_data = cases_data.str.lower()\n",
        "# Remove extra spaces\n",
        "cases_data = cases_data.str.split().str.join(' ') # Assign the result back to cases_data\n",
        "cases_data.head()\n",
        "\n",
        "print(len(cases_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "087zICyX2npb"
      },
      "source": [
        "### Apply tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8rGmhuLJGOL"
      },
      "outputs": [],
      "source": [
        "# install libraries for tokenisation\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EgI0oNduJXSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635b66ed-f053-4011-e354-ca12e11db98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens DataFrame shape: (93686, 512)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the tokenizer once\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"obaidtambo/hinglish_bert_tokenizer\")\n",
        "\n",
        "# Apply tokenization\n",
        "def apply_tokenisation(text):\n",
        "    if isinstance(text, pd.Series):\n",
        "        text = text.astype(str).tolist()  # Convert to a list of strings\n",
        "    elif isinstance(text, str):\n",
        "        text = [text]  # Convert single text to list for consistency\n",
        "\n",
        "    # Perform tokenization in a single batch\n",
        "    encoded_output = tokenizer(\n",
        "        text,\n",
        "        return_tensors=None,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    # Extract tokens and token IDs for all sentences in the batch\n",
        "    all_token_ids = encoded_output['input_ids']\n",
        "    return all_token_ids\n",
        "\n",
        "# Tokenize the entire dataset as a batch\n",
        "tokens = apply_tokenisation(cases_data)\n",
        "\n",
        "# Convert to DataFrame if you want an easy view\n",
        "tokens_df = pd.DataFrame(tokens)\n",
        "print(\"Tokens DataFrame shape:\", tokens_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u3jSHKaMssg"
      },
      "source": [
        "### Remove Stopwords: English and Hinglish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BK4IZLLfd30"
      },
      "outputs": [],
      "source": [
        "# Install used libraries\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Rtfd0W_-fHUG"
      },
      "outputs": [],
      "source": [
        "# import libraries for removing stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU1HagvqAjJ-"
      },
      "source": [
        "Import a file containing Hinglish stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MwG5vZcv08Ud"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from typing import Set, Optional\n",
        "import pandas as pd\n",
        "\n",
        "class HinglishStopwordsLoader:\n",
        "    def __init__(self):\n",
        "        self.github_raw_url = \"https://raw.githubusercontent.com/TrigonaMinima/HinglishNLP/master/data/assets/stop_hinglish\"\n",
        "        self.stopwords: Set[str] = set()\n",
        "\n",
        "    def load_stopwords(self, cache: bool = True) -> Set[str]:\n",
        "        \"\"\"\n",
        "        Load Hinglish stopwords from GitHub\n",
        "\n",
        "        Args:\n",
        "            cache: Whether to cache the stopwords after first load\n",
        "\n",
        "        Returns:\n",
        "            Set of Hinglish stopwords\n",
        "        \"\"\"\n",
        "        # Return cached stopwords if available\n",
        "        if cache and self.stopwords:\n",
        "            return self.stopwords\n",
        "\n",
        "        try:\n",
        "            # Fetch content from GitHub\n",
        "            response = requests.get(self.github_raw_url)\n",
        "            response.raise_for_status()  # Raise exception for bad status codes\n",
        "\n",
        "            # Process the content\n",
        "            stopwords = set(\n",
        "                word.strip()\n",
        "                for word in response.text.split('\\n')\n",
        "                if word.strip()  # Remove empty lines\n",
        "            )\n",
        "\n",
        "            if cache:\n",
        "                self.stopwords = stopwords\n",
        "\n",
        "            return stopwords\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching stopwords: {e}\")\n",
        "            return set()\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove stopwords from given text\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            Text with stopwords removed\n",
        "        \"\"\"\n",
        "        if not self.stopwords:\n",
        "            self.load_stopwords()\n",
        "\n",
        "        words = text.lower().split()\n",
        "        return ' '.join(word for word in words if word not in self.stopwords)\n",
        "\n",
        "    def get_stopwords_df(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Return stopwords as a DataFrame for easy viewing\n",
        "        \"\"\"\n",
        "        stopwords = self.load_stopwords()\n",
        "        return pd.DataFrame(sorted(list(stopwords)), columns=['Stopwords'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4iuVZjjH3fYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27afb53e-da9c-4e1a-cb92-0a4e90c2327f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Hinglish Stop words\n",
        "loader = HinglishStopwordsLoader()\n",
        "\n",
        "# Initialize stopwords\n",
        "english_stopwords = set(stopwords.words(\"english\"))\n",
        "hindi_stopwords = loader.load_stopwords()\n",
        "# custom_stopwords = {\"maine\", \"ke\", \"par\", \"ka\", \"ki\", \"hai\"}  # Add common Hinglish stopwords\n",
        "\n",
        "# Merge stopwords into a single set\n",
        "all_stopwords = english_stopwords | hindi_stopwords  # | custom_stopwords\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(token_ids_list):\n",
        "    # Decoding all tokens in batches\n",
        "    decoded_tokens = [tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=True) for token_ids in token_ids_list]\n",
        "\n",
        "    # Vectorized stopword filtering using a list comprehension\n",
        "    filtered_token_ids = [\n",
        "        tokenizer.convert_tokens_to_ids([token for token in tokens if token not in all_stopwords])\n",
        "        for tokens in decoded_tokens\n",
        "    ]\n",
        "    return filtered_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIoehk2wghc5",
        "outputId": "3eba7864-1885-4e53-c399-3f4998d5a443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens DataFrame shape: (93686, 489)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess and print result\n",
        "filtered_tokens = remove_stopwords(tokens)\n",
        "\n",
        "# Convert to DataFrame if needed\n",
        "filtered_tokens_df = pd.DataFrame(filtered_tokens)\n",
        "print(\"Filtered Tokens DataFrame shape:\", filtered_tokens_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAl-tj5NQ5Kb"
      },
      "source": [
        "# Step 2: Feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfR7C-IvkrZq"
      },
      "source": [
        "For textual data, we will convert the data into numerical form and use TFIDF to extract the most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "o6DZAjkmj2IJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe745f3-7c74-4094-ddae-ef54a5d38f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Feature Matrix shape: (93686, 15999)\n"
          ]
        }
      ],
      "source": [
        "# Use TF-IDF to convert tokens into numerical features or matrices\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assume filtered_tokens is the output from the stopword removal process\n",
        "# Step 1: Reconstruct the text from the filtered tokens\n",
        "reconstructed_texts = [' '.join(tokenizer.convert_ids_to_tokens(token_ids)) for token_ids in filtered_tokens]\n",
        "\n",
        "# Step 2: Use TF-IDF to convert the reconstructed texts into numerical features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(reconstructed_texts)\n",
        "\n",
        "# Check the shape of the resulting TF-IDF features\n",
        "print(\"TF-IDF Feature Matrix shape:\", tfidf_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YsveVlck3Jh"
      },
      "source": [
        "For labelled or categorical data, we will convert it into encodings or numerical values that our classifier can interpret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "U6IID1ullCvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57e4298-a32b-42a0-c9ba-d499f4774a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories shape: 93686\n",
            "(93686,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Example categories\n",
        "categories = df['sub_category']\n",
        "print(\"Categories shape:\", len(categories))\n",
        "# [\"UPI Related Frauds\", \"Internet Banking Related Fraud\", \"DebitCredit Card FraudSim Swap Fraud\", \"Cyber Bullying Stalking Sexting\"]\n",
        "\n",
        "# Label encoding the categories\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(categories)\n",
        "print(encoded_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy5aETWrNarG"
      },
      "source": [
        "# Step 3: Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsunSJm5lLkn"
      },
      "source": [
        "To train the model, first we will split our textual and categorical data into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kgmKzRVNQrwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1570e3-b925-490c-cf0a-ee8319dd4883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (74948, 15999) (74948,)\n",
            "Test set shape: (18738, 15999) (18738,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Step 4: Split the data\n",
        "# Assuming `tfidf_features` is the feature matrix from text and `encoded_labels` are the encoded categories\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN3Jx4wxlhWv"
      },
      "source": [
        "#### Initialize and Train the LinearSVC Model\n",
        "Now that you have both text features and encoded labels, train the LinearSVC model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "h3mJ9nw0loMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7751f52e-f339-4831-e8fb-aef6a97c4bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.5484043120930729\n",
            "Cross-validated accuracy scores: [0.55046537 0.54427461 0.55142821 0.55194057]\n",
            "Average accuracy: 0.5495271899151611\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "# Initialize LinearSVC\n",
        "clf = LinearSVC(C=0.1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test model accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Model accuracy:\", accuracy)\n",
        "\n",
        "kf = KFold(n_splits=4)\n",
        "scores = cross_val_score(clf, tfidf_features, encoded_labels, cv=kf)\n",
        "print(\"Cross-validated accuracy scores:\", scores)\n",
        "print(\"Average accuracy:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example to predict the category of text with the model"
      ],
      "metadata": {
        "id": "CR57t_zcbefj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text to classify\n",
        "sample_text = \"\"\"\n",
        "I would like to complain against EASY BORROW app on Google Playstore\n",
        "These apps collect data from their users phone like contacts photos etc and\n",
        "then they call on those numbers for payment They use foul languages and harass them\n",
        "This is a serious cfrime of stealing data from mobile phones and using it to bully the\n",
        "users I request you to look into\n",
        "this and take strict action against such apps and companies supporting them Thank You\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Preprocess the sample text\n",
        "def preprocess_sample_text(text):\n",
        "    # Tokenization (using the same tokenizer as before)\n",
        "    encoded_output = tokenizer.encode_plus(\n",
        "        text,\n",
        "        return_tensors=None,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    token_ids = encoded_output.get('input_ids')\n",
        "\n",
        "    # Remove stopwords from token_ids\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=True)\n",
        "    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
        "\n",
        "    # Reconstruct the text for TF-IDF vectorization\n",
        "    reconstructed_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    # Convert to TF-IDF features\n",
        "    tfidf_vector = tfidf_vectorizer.transform([reconstructed_text])\n",
        "\n",
        "    return tfidf_vector\n",
        "\n",
        "# Step 2: Process the sample text and get TF-IDF features\n",
        "sample_tfidf = preprocess_sample_text(sample_text)\n",
        "\n",
        "# Step 3: Predict the category\n",
        "predicted_label = clf.predict(sample_tfidf)\n",
        "\n",
        "# Step 4: Decode the predicted label back to original category\n",
        "predicted_category = label_encoder.inverse_transform(predicted_label)\n",
        "\n",
        "print(\"Predicted Category:\", predicted_category[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7FFYWhNba7E",
        "outputId": "18de5316-3442-4ba8-b04b-9503459252ae"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: Cyber Bullying  Stalking  Sexting\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZhkBGx8E2Y5aoE9Bt/4HU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}